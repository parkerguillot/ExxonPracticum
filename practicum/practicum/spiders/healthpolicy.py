# -*- coding: utf-8 -*-
# These are the required packages need for this python file to run completely
import scrapy
from datetime import datetime

# References the items file which is where the user specifies the items that need to be scraped
from ..items import PracticumItem

# We need to define a start url or the website we are interested in scraping
urls = 'http://healthpolicyandmarket.blogspot.com/'

# Defining the class statement to be executed. This contains the whole scraping process
class HealthPolicySpider(scrapy.Spider):
    # name = declaring the name of spider for later use in program
    name = 'healthpolicy'

    # establish the earliest blogs we would like to scrape. The format for this website has blogs
    # separated by year which is how we will scrape blogs up until the present
    year = 2012

    # This first function starts the request y loading the provided url above
    def start_requests(self):
        yield scrapy.Request(url= urls,
                             callback=self.pages)

    # First parsing method. Scrapes the first Web page generated by the start_requests function.
    # This function will then call parse_front to collect the blog links.
    def pages(self,response):
        # start_url starts at the first blog which is referenced by the counter(year)
        start_url = 'http://healthpolicyandmarket.blogspot.com/' + str(HealthPolicySpider.year) + '/'
        yield response.follow(start_url, callback=self.parse_front)

    # Second parsing method. Scrapes the first Web page generated by the start_requests function.
    def parse_front(self, response):
        # article_links are the urls corresponding to each blog post. These need to be collected
        # to "go into" the blog post.
        article_links = response.xpath('//h3[@class="post-title entry-title"]/a/@href')
        # extracts the links from the scraped article_links
        links_to_follow = article_links.extract()

        # This will loop through each link in the list of blog links. The response will "click"
        # on each blog to go into the blog post. Then the function will then call the parse_pages function
        for link in links_to_follow:
            yield response.follow(url=link,
                                  callback=self.parse_pages)

        # this if statement will happen if the counter(year) is less than the specified year.
        # If the counter is already at the specified year, the if statement will not happen and the
        # program will end.
        if HealthPolicySpider.year < 2020:
            HealthPolicySpider.year += 1

            # This will be the next url to scrape which contains more blogs referenced by the year
            # the blogs were published. This function will then go to the url and call the parse_front function.
            next_page = 'http://healthpolicyandmarket.blogspot.com/' + str(HealthPolicySpider.year) + '/'
            print(HealthPolicySpider.year)
            yield response.follow(next_page, callback=self.parse_front)

    # the parse_pages function collects the information from the individual blog posts.
    def parse_pages(self, response):

        # calls the items.py file. The items file specifies which data points are being collected
        items = PracticumItem()

        # These are the locators for this particular blog website. These locators can be found
        # in the HTML text of the website. these locators are specific for this website
        twitter = []
        article_date = response.xpath('//h2[(@class="date-header")]').re(r"\w+\s\d{1,2},\s\d{4}")
        article_title = response.xpath('//h3[(@class = "post-title entry-title")]//text()').extract()
        author = response.xpath('//span[(@itemprop="name")]//text()').re(r"[A-Z]\w+\s\w+")
        article_text = response.xpath('//div[(@class="post-body entry-content")]//text()').extract()

        # The article text is sometimes in separate paragraphs so this "body" function
        # will combine the paragraphs into one
        body = ''
        for text in article_text:
            body = body + text
        # These 3 lines clean up the body by removing new lines, tabs, and carriage return
        body = body.replace('\r', '')
        body = body.replace('\n', '')
        body = body.replace('\t', '')

        # Error handling if certain extracts do not catch anything. his way we insert a blank string
        # so we can still insert the record into the database
        if len(twitter) > 0:
            items['twitter'] = twitter[0]
        else:
            items['twitter'] = ''
        if len(article_date) > 0:
            items['article_date'] = article_date[0]
        else:
            items['article_date'] = ''
        if len(author) > 0:
            items['author'] = author[0]
        else:
            items['author'] = ''
        if len(article_title) > 0:
            items['article_title'] = article_title[0]
        else:
            items['article_title'] = ''
        items['article_url'] = response.request.url
        items['article_text'] = body
        items['timestamp'] = datetime.now()

        # The yield items will yield the extracted items and put through the pipeline.
        # The pipeline was specified in the settings file by uncommenting the code below:
        # ITEM_PIPELINES = {
        #     'practicum.pipelines.PracticumPipeline': 300,
        # }
        yield items