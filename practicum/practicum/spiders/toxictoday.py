# -*- coding: utf-8 -*-

# These are the required packages need for this python file to run completely
import scrapy
from ..items import ExxonItem
from datetime import datetime

# We need to define a start url or the website we are interested in scraping
urls = 'http://thetoxicologisttoday.blogspot.com/'

# Defining the class statement to be exceuted. This contains the whole scraping process
class SpideySensesAreTingling(scrapy.Spider):
        name = 'toxictoday'

        # This first function starts the request y loading the provided url above
        def start_requests(self):
            yield scrapy.Request(url=urls,
                                 callback=self.parse_front)

        # First parsing method. Scrapes the first Web page generated by the start_requests function.First parsing method
        def parse_front(self, response):
            next_page = response.xpath('//a[contains(@class, "older")]//@href').get()

            # article_links are the urls corresponding to each blog post. These need to be collected
            # to "go into" the blog post.
            article_links = response.xpath('//h3[@class = "post-title entry-title"]/a/@href')

            # extracts the links from the scraped article_links
            links_to_follow = article_links.extract()

            # This will loop through each link in the list of blog links. The response will "click"
            # on each blog to go into the blog post and the function will then call the parse_pages function
            for link in links_to_follow:
                yield response.follow(url=link,
                                      callback=self.parse_pages)

            if next_page is not None:
                yield response.follow(next_page, callback=self.parse_front)

        # the parse_pages function collects the information from the individual blog posts.
        def parse_pages(self, response):

            # calls the items.py file. The items file specifies which data points are being collected
            items = ExxonItem()

            # looks at source code but only at a certain html tag in the document
            # html tag for quotes is the div (tag) which is why it is specified first

            # These are the locators for this particular blog website. These locators can be found
            # in the HTML text of the website. these locators are specific for this website
            twitter = []
            article_date = response.xpath('//h2[@class = "date-header"]//text()').extract()
            article_title = response.xpath('//h3[@class = "post-title entry-title"]//text()').extract()
            author = response.xpath('//span[@itemprop = "name"]/text()').extract()
            article_text = response.xpath('//div[@style = "text-align: justify;"]//text()').extract()

            # The article text is sometimes in seperate paragraphs so this "body" function
            # will combine the paragraphs into one
            body = ''
            for text in article_text:
                body = body + text

            # Error handling if certain extracts do not catch anything. his way we insert a blank string
            # so we can still insert the record into the database
            if len(twitter) > 0:
                items['twitter'] = twitter[0]
            else:
                items['twitter'] = ''
            if len(author) > 0:
                items['author'] = author[0]
            else:
                items['author'] = ''
            if len(article_date) > 0:
                items['article_date'] = article_date[0]
            else:
                items['article_date'] = ''
            if len(article_date) > 0:
                items['article_title'] = article_title[0]
            else:
                items['article_title'] = ''

            # These 3 lines clean up the body by removing new lines, tabs, and carriage return
            body = body.replace('\r', '')
            body = body.replace('\n', '')
            body = body.replace('\t', '')
            # region = region.replace('\r', '')
            # region = region.replace('\n', '')
            # region = region.replace('\t', '')
            # author = author.replace('\r', '')
            # author = author.replace('\n', '')
            # author = author.replace('\t', '')

            # declares items extracted for each of the following
            items['article_url'] = response.request.url
            # items['article_date'] = article_date
            # items['twitter'] = twitter
            # items['article_title'] = article_title
            # items['author'] = author
            items['article_text'] = body
            items['timestamp'] = datetime.now()

            yield items